<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>分布式 - 分类 - lack 的个人博客</title>
        <link>http://xingyys.tech/categories/%E5%88%86%E5%B8%83%E5%BC%8F/</link>
        <description>分布式 - 分类 - lack 的个人博客</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 10 Jan 2021 14:34:22 &#43;0800</lastBuildDate><atom:link href="http://xingyys.tech/categories/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="self" type="application/rss+xml" /><item>
    <title>分布式的工作流实现</title>
    <link>http://xingyys.tech/posts/linux/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%AE%9E%E7%8E%B0/</link>
    <pubDate>Sun, 10 Jan 2021 14:34:22 &#43;0800</pubDate><author>
        <name>Lack</name>
    </author><guid>http://xingyys.tech/posts/linux/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%AE%9E%E7%8E%B0/</guid>
    <description><![CDATA[本篇提供一个实现分布式工作流的思路。
系统组成部分：
 api (网关接口) : 为用户提供工作流的api接口 discovery (服务发现) : 用于服务的注册和发现 scheduler (调度中心) : 整体工作流的调度 broker (消息队列) : 订阅发布和数据传输 node (工作节点) : 每个工作单元的提供者和执行者  实现思路：
 node 启动时上传每个工作单元的基本信息: 包含工作单元输入输出、名称及其他内容 scheduler 保存这些信息并提供显示 scheduler 接收来自前端的数据并转化成工作流的信息，并执行  实现细节：
 scheduler 动态生成工作流信息，在 broker 中启动 topic 一个订阅者接收工作流的动态信息 scheduler 根据工作单元信息寻找 node 并传输信息 node 执行对应的工作单元发布信息到 broker 中工作流 topic scheduler 接收 topic 中的信息，如果中间发生错误，执行回滚操作 node 需要的输入参数保存在 context.Context 中，返回参数则 publish 到 topic 中  // 工作单元的接口 type WorkUnit interface { Do(ctx) error Undo(ctx) error String() string } type workflow struct{ units []WorkUnit } func Builder(ctx context.]]></description>
</item><item>
    <title>etcd的使用实例</title>
    <link>http://xingyys.tech/posts/linux/etcd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B/</link>
    <pubDate>Wed, 11 Sep 2019 15:17:42 &#43;0800</pubDate><author>
        <name>作者</name>
    </author><guid>http://xingyys.tech/posts/linux/etcd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B/</guid>
    <description><![CDATA[etcd 有如下的使用场景：
 服务发现（Service Discovery） 消息发布与订阅 负载均衡 分布式通知与协调 分布式锁 分布式队列 集群监控于Leader竞选。  一、服务发现 etcd 的常见使用场景之一就是服务发现。实现思路如下：先准备 etcd 服务端，服务端的程序在第一次启动之后会连接到 etcd 服务器并设置一个格式为 ip:port 的键值对，并绑定一个 lease。之后的服务端内部维护一个定时器，每隔一段时间就更新服务端注册中心的 lease 的 TTL。另外一个组件就是服务发现组件，discovery 会 watch 服务端的 key。每次该 key 变化时，discovery 就可以检测到时间并做出对应的操作。代码的实现如下：
// server.go package main import ( &#34;context&#34; &#34;crypto/md5&#34; &#34;encoding/json&#34; &#34;errors&#34; &#34;flag&#34; &#34;fmt&#34; &#34;github.com/coreos/etcd/clientv3&#34; &#34;github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes&#34; &#34;log&#34; &#34;net&#34; &#34;os&#34; &#34;os/signal&#34; &#34;strings&#34; &#34;syscall&#34; &#34;time&#34; ) var ( prefix = &#34;register&#34; client *clientv3.Client stopSignal = make(chan struct{}, 1) srvKey string ) var ( serv = flag.]]></description>
</item><item>
    <title>Go 结合 etcd</title>
    <link>http://xingyys.tech/posts/go/go%E7%BB%93%E5%90%88etcd/</link>
    <pubDate>Tue, 10 Sep 2019 15:17:42 &#43;0800</pubDate><author>
        <name>Lack</name>
    </author><guid>http://xingyys.tech/posts/go/go%E7%BB%93%E5%90%88etcd/</guid>
    <description><![CDATA[关于 etcd 的安装和介绍看 这里 。官方的实例可以看 这里
一、连接 首先是关于 golang 如何连接 etcd ，先是简单的连接。
package main import ( &#34;github.com/coreos/etcd/clientv3&#34; &#34;log&#34; &#34;time&#34; ) func connect() { cli, err := clientv3.New(clientv3.Config{ // etcd 集群的地址集合 	Endpoints: []string{&#34;192.168.10.10:2379&#34;}, // 请求超时时间 	DialTimeout: time.Second * 3, }) if err != nil { log.Fatal(&#34;connect etcd cluster: &#34; + err.Error()) } cli.Close() } 还有带 https 和 开启用户验证的连接
func connectTlsAuth() { tlsInfo := transport.TLSInfo{ CertFile: &#34;/tmp/cert.pem&#34;, KeyFile: &#34;/tmp/key.pem&#34;, TrustedCAFile: &#34;/tmp/ca.]]></description>
</item></channel>
</rss>
