---
title: "Io多路复用"
date: 2022-05-16T15:10:34+08:00
draft: true
---

# select

第一种是select。
我们可以设置要等待的描述符，也可以设置等待超时时间。如果有准备好的fd，或达到指定超时时间，select函数就会返回。


![](https://raw.githubusercontent.com/yzj0911/my_logs/main/content/images/select1.png)


从函数签名来看，它支持监听可读、可写、异常三类事件。
因为这个fd_set是个unsigned long型的数组。共16个元素，每一位对应一个fd，最多可以监听1024个，这就有点少了。
而且每次调用select都要传递所有的监听集合。这就需要频繁的从用户态到内核拷贝数据。除此之外，即便有fd就绪了，也需要遍历整个监听集合，来判断哪个fd是可操作的。这些都会影响性能。

![](https://raw.githubusercontent.com/yzj0911/my_logs/main/content/images/select2.png)



# poll

第二种IO多路复用的实现方式是poll。
虽然支持的fd数目，等于最多可以打开的文件描述符个数。但是另外两个问题依然存在。


# epoll

而epoll就没有这些问题了，它提供三个接口。

epoll_create1用于创建一个epoll，并获取一个句柄。
epoll_ctl用于添加或删除fd与对应的事件信息。
除了指定fd和要监听的事件类型，还可以传入一个event data，通常会按需定义一个数据结构，用于处理对应的fd。可以看到，每次都只需传入要操作对的一个fd，无需传入所有监听集合，而且只需要注册这一次。通过epoll_wait得到的fd集合都是以及就绪的，逐个处理即可，无需遍历所有监听集合。

![](https://raw.githubusercontent.com/yzj0911/my_logs/main/content/images/epllo1.png)


通过IO多路复用，线程再也不用为等待某一个socket，而阻塞或空耗CPU。并发处理能力因而大幅提升。


IO多路复用结合协程

但是IO多路复用也并非没有问题，例如：一个socket可读了，但是这回只读到了半条请求，也就是说需要再次等待这个socket可读。在继续处理下一个socket之前，需要记录下这个socket的处理状态。下一次这个socket可读时，也需要恢复上次保存的现场，才好继续处理。
也就是说，在IO多路复用中实现业务逻辑时，我们需要随着事件的等待和就绪，而频繁的保存和恢复现场，这并不符合常规的开发习惯。如果业务逻辑比较简单还好，若是比较复杂的业务场景，就有些悲剧了。

![](https://raw.githubusercontent.com/yzj0911/my_logs/main/content/images/202205161.png)



既然业务处理过程中，要等待事件时，需要保存现场并切换到下一个就绪的fd。而事件就绪时，又需要恢复现场继续处理。那岂不是很适合使用协程？


在IO多路复用这里，事件循环依然存在，依然要在循环中逐个处理就绪的fd，但处理过程却不是围绕具体业务，而是面向协程调度。
如果是用于监听端口的fd就绪了，就建立连接创建一个新的fd，交给一个协程来负责, 协程执行入口就指向业务处理函数入口，业务处理过程中，需要等待时就注册IO事件，然后让出，这样，执行权就会回到切换到该协程的地方继续执行。如果是其它等待IO事件的fd就绪了，只需要恢复关联的协程即可。

协程拥有自己的栈，要保存和恢复现场都很容易实现。这样，IO多路复用这一层的事件循环，就和具体业务逻辑解耦了。

可以把read、write、connect等可能会发生等待的函数包装一下，在其中实现IO事件注册与主动让出。这样在业务逻辑层面，就可以使用这些包装函数，按照常规的顺序编程方式，来实现业务逻辑了。

这些包装函数在需要等待时，就会注册IO事件，然后让出协程，这样我们在实现业务逻辑时，就完全不用担心保存与恢复现场的问题了。


![](https://raw.githubusercontent.com/yzj0911/my_logs/main/content/images/202205162.png)


协程和IO多路复用之间的合作，不仅保留了IO多路复用的高并发性能，还解放了业务逻辑的实现。


